{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"this is nlp program which i am doing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from click->nltk) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from importlib-metadata->click->nltk) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from importlib-metadata->click->nltk) (4.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: click in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     -------------------------------------- 77.1/77.1 kB 718.9 kB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-win_amd64.whl (268 kB)\n",
      "     -------------------------------------- 268.0/268.0 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from click->nltk) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from importlib-metadata->click->nltk) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\prave\\desktop\\nlp_rnn_lstm\\sentiment_analysis\\venv\\lib\\site-packages (from importlib-metadata->click->nltk) (4.5.0)\n",
      "Installing collected packages: tqdm, regex, joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1 regex-2022.10.31 tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is nlp program which i am doing.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning .Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At eight o'clock on Thursday morning .Arthur didn't feel very good.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"At eight o'clock on Thursday morning .Arthur didn't feel very good.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " '.Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming to find based root node form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('finally')# comming back to root node of the word ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmitization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm=WordNetLemmatizer()\n",
    "lemm.lemmatize(\"going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prave\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n",
      "['t']\n",
      "[]\n",
      "['e']\n",
      "['i']\n",
      "['g']\n",
      "['h']\n",
      "['t']\n",
      "[]\n",
      "['o']\n",
      "[\"'\"]\n",
      "['c']\n",
      "['l']\n",
      "['o']\n",
      "['c']\n",
      "['k']\n",
      "[]\n",
      "['o']\n",
      "['n']\n",
      "[]\n",
      "['t']\n",
      "['h']\n",
      "['u']\n",
      "['r']\n",
      "['s']\n",
      "['d']\n",
      "['a']\n",
      "['y']\n",
      "[]\n",
      "['m']\n",
      "['o']\n",
      "['r']\n",
      "['n']\n",
      "['i']\n",
      "['n']\n",
      "['g']\n",
      "[]\n",
      "['.']\n",
      "['a']\n",
      "['r']\n",
      "['t']\n",
      "['h']\n",
      "['u']\n",
      "['r']\n",
      "[]\n",
      "['d']\n",
      "['i']\n",
      "['d']\n",
      "['n']\n",
      "[\"'\"]\n",
      "['t']\n",
      "[]\n",
      "['f']\n",
      "['e']\n",
      "['e']\n",
      "['l']\n",
      "[]\n",
      "['v']\n",
      "['e']\n",
      "['r']\n",
      "['y']\n",
      "[]\n",
      "['g']\n",
      "['o']\n",
      "['o']\n",
      "['d']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[stemmer.stem(word) for word in words]\n",
    "    print(words)\n",
    "    corpus.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a'],\n",
       " ['t'],\n",
       " [],\n",
       " ['e'],\n",
       " ['i'],\n",
       " ['g'],\n",
       " ['h'],\n",
       " ['t'],\n",
       " [],\n",
       " ['o'],\n",
       " [\"'\"],\n",
       " ['c'],\n",
       " ['l'],\n",
       " ['o'],\n",
       " ['c'],\n",
       " ['k'],\n",
       " [],\n",
       " ['o'],\n",
       " ['n'],\n",
       " [],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['u'],\n",
       " ['r'],\n",
       " ['s'],\n",
       " ['d'],\n",
       " ['a'],\n",
       " ['y'],\n",
       " [],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['n'],\n",
       " ['i'],\n",
       " ['n'],\n",
       " ['g'],\n",
       " [],\n",
       " ['.'],\n",
       " ['a'],\n",
       " ['r'],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['u'],\n",
       " ['r'],\n",
       " [],\n",
       " ['d'],\n",
       " ['i'],\n",
       " ['d'],\n",
       " ['n'],\n",
       " [\"'\"],\n",
       " ['t'],\n",
       " [],\n",
       " ['f'],\n",
       " ['e'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " [],\n",
       " ['v'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['y'],\n",
       " [],\n",
       " ['g'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['.']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN'),\n",
       " ('.Arthur', 'NN'),\n",
       " ('did', 'VBD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('feel', 'VB'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\prave/nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\prave\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12536\\4271528683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmaxent_ne_chunker\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('maxent_ne_chunker')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mchunkers/maxent_ne_chunker/english_ace_multiclass.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\prave/nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\prave\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mtreebank\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('treebank')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/treebank/combined\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\prave/nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\prave\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtreebank\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('treebank')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/treebank.zip/treebank/combined/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\prave/nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\prave\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12536\\3250786591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtreebank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsed_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wsj_0001.mrg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prave\\Desktop\\NLP_RNN_LSTM\\Sentiment_Analysis\\venv\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtreebank\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('treebank')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/treebank/combined\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\prave/nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\prave\\\\Desktop\\\\NLP_RNN_LSTM\\\\Sentiment_Analysis\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\prave\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
